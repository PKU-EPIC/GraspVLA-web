<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="css/index.css">
  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family: 'Open Sans', sans-serif;
    }
  </style>

</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container-fluid">
      <div class="row">
        <div class="col">
          <h2 style="font-size:45px;">GraspVLA: a Grasping Foundation Model <br>
            Pre-trained on Billion-scale Synthetic Action Data</h2>
          <!-- <h4 style="color:#6e6e6e;"> In submission </h4> -->
          <hr>
          <h6>
            <a href="https://shengliangd.github.io/about/">Shengliang Deng</a><sup>1,3*</sup>&nbsp; &nbsp;
            <a href="https://miyandoris.github.io/">Mi Yan</a><sup>1,2*</sup>&nbsp; &nbsp;
            <a href="https://songlin.github.io/">Songlin Wei</a><sup>1,2</sup>&nbsp; &nbsp;
            <a>Haixin Ma</a><sup>1</sup>&nbsp; &nbsp;
            <a>Yuxin Yang</a><sup>1</sup>&nbsp; &nbsp; <br>
            <a href="https://jychen18.github.io/">Jiayi Chen</a><sup>1,2</sup>&nbsp; &nbsp;
            <a href="https://i.cs.hku.hk/~heming/">Heming Cui</a><sup>3</sup>&nbsp; &nbsp;
            <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a><sup>1,4</sup>&nbsp; &nbsp;
            <a href="https://hughw19.github.io/">He Wang</a><sup>1,2,4†</sup>
            <br>
            <br>
            <p> <sup>1</sup>Galbot&nbsp; &nbsp;
              <sup>2</sup>Peking University&nbsp; &nbsp;
              <sup>3</sup>The University of Hong Kong&nbsp; &nbsp;
              <sup>4</sup>Beijing Academy of Artificial Intelligence&nbsp; &nbsp;
              <br>
            </p>
            <p>
              <sup>†</sup> corresponding author &nbsp;
              <br>
            </p>

            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2401.07745" role="button" target="_blank" style="background-color: #d3d3d3d2;color: black">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/MaskClustering" role="button" target="_blank" disabled=1 style="background-color: #d3d3d3d2;color: black">
              <i class="fa fa-github-alt"></i> Checkpoint</a> </p>
            </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/MaskClustering" role="button" target="_blank" disabled=1 style="background-color: #d3d3d3d2;color: black">
                <i class="fa fa-github-alt"></i> Code</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/MaskClustering" role="button" target="_blank" disabled=1 style="background-color: #d3d3d3d2;color: black">
              <i class="fa fa-github-alt"></i> Dataset</a> </p>
              </div>
            </div>

            <!-- <div>
              <br>
              <br>
              <strong>CVPR 2024</strong>
            </div> -->

        </div>
      </div>
    </div>
  </div>
</section>

<!-- teaser -->
<section>
  <div class="container" style="width:70%" id="teaser">
    <img src="images/teaser.jpg" width="100%"> 
  </div>
</section>
<br>

<!-- abstract -->
<section>
  <div class="container" style="width:70%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Abstract</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          In the era of large models, increasing efforts have been dedicated to developing embodied foundation models for general-purpose applications. They enable zero-shot generalization, exhibit scaling behaviors, and support few-shot post-training for specialized demands. However, their performance relies heavily on large-scale robotic data, which is challenging to collect at scale. We find that leveraging large-scale synthetic robotic data offers a scalable and cost-efficient method for building embodied foundation models, yet it has previously been underestimated. In this paper, we introduce GraspVLA, a Vision-Language-Action (VLA) model for grasping, pretrained on large-scale synthetic action data as a foundational model for this essential skill. To achieve this, we first curate a billion-scale grasping dataset in simulation, featuring photorealistic rendering and extensive domain randomization. We further propose an effective co-training strategy to integrate both perception and control tasks into a unified next-token-prediction training paradigm. This enables the unprecedented strong generalizability of GraspVLA across various factors, including spatial positions, backgrounds, distractions, and lighting variations in real-world environments. Besides, this generalizability can be scalable in two key dimensions: first, its zero-shot generalizability improves with increasing data scale; second, it can generalize to novel categories by being co-trained with grounding tasks using Internet data. In addition, we also showcase that GraspVLA can support a few-shot post-training to acquire complex grasping patterns aligned with human preference. We perform extensive experiments to highlight GraspVLA's superior generalizability, examine its scaling behaviors, and illustrate its potential for few-shot post-training.  
        </p>
      </div>
    </div>
  </div>
</section>
<br>

<!-- zero-shot -->
<section>
  <div class="container" style="width:70%" id="zero-shot-video">
    <div class="row">
      <div class="col-12">
        <h2><strong>Zero-Shot Evaluation</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          Pre-trained on our billion-scale dataset, GraspVLA demonstrates strong zero-shot generalizability across 6 aspects, including distractor, spatial pose, category, lighting, background, and close-loop actions.
        </p>

        <h5><strong>1. Generalization to distractors</strong></h5>
        <p class="text-justify">
          Cluttered scenes with 30+ distractors.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/1-dense.mp4" type="video/mp4">
          </video>
        </div>
        <br>
        <p class="text-justify">
          Dynamic distractors.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/1-distractor.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>2. Generalization to lighting variations</strong></h5>
        <p class="text-justify">
          Various lighting conditions. In the second video, even when the object is moved to a new location under dark lighting conditions, GraspVLA can still track and grasp it.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/2-light.mp4" type="video/mp4">
          </video>
        </div>
        <br>
        <br>

        <h5><strong>3. Generalization to spatial variations</strong></h5>
        <p class="text-justify">
          Balls at different heights.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/3-height.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <p class="text-justify">
          Eggs with different planar poses.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/3-planar.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>4. Generalization to background variations</strong></h5>
        <p class="text-justify">
          Table with different textures.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/4-background-desk.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <p class="text-justify">
          Wall with changing colors.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/4-background-wall.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>5. Generalization to categories</strong></h5>
        <p class="text-justify">
          Co-trained with grounding tasks using Internet data, GraspVLA can generalize to novel categories without any action label.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/5-category.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>6. Closed-loop control</strong></h5>
        <p class="text-justify">
          GraspVLA can automatically make closed-loop adjustments in response to disturbances until the task is completed.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="40%">
            <source src="videos/6-closed-loop.mp4" type="video/mp4">
          </video>
        </div>
        <br>

      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>

<!-- post-train -->
<section>
  <div class="container" style="width:70%" id="zero-shot-video">
    <div class="row">
      <div class="col-12">
        <h2><strong>Efficient post-training</strong></h2>
        <hr style="margin-top:0px">

        <h5><strong>1. Industry: new vocabulary</strong></h5>
        <p class="text-justify">
          In industrial scenarios, although our pre-trained GraspVLA can grasp any part, it struggles to identify parts with special names.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="40%">
            <source src="videos/industry-fail.mp4" type="video/mp4">
          </video>
        </div>
        <br>
        <p class="text-justify">
          A few data with only bounding box annotation helps GraspVLA master all the rare parts.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/industry.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>2. Retail: novel action patterns</strong></h5>
        <p class="text-justify">
          In retail scenario, with a few trajectories on one kind of bottles, GraspVLA learns to sequentially pick up bottles from a densely-packed environment. This behavior can also be transferred to bottles unseen in post-training.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/retail.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>3. Home: novel action patterns</strong></h5>
        <p class="text-justify">
          In retail scenario, with a few trajectories on one kind of bottles, GraspVLA learns to sequentially pick up bottles from a densely-packed environment. This behavior can also be transferred to bottles unseen in post-training.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/retail.mp4" type="video/mp4">
          </video>
        </div>
        <br>


      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>


<!-- Contact -->
<div class="container" style="width:70%">
  <div class="row ">
    <div class="col-12">
      <h2><strong>7. Contact</strong></h2>
      <hr style="margin-top:0px">
      <p>If you have any questions, please feel free to contact <b>Shengliang Deng</b> at shengliangd@outlook.com and <b>Mi Yan</b> at dorisyan@pku.edu.cn.
      </p>
      </pre>
    </div>
  </div>
</div>

<footer class="text-center" style="margin-bottom:10px; font-size: medium;">
  <hr>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a
    href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
</footer>

</body>
</html>