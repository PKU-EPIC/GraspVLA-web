<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="css/index.css">
  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family: 'Open Sans', sans-serif;
    }
  </style>

</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container-fluid">
      <div class="row">
        <div class="col">
          <h2 style="font-size:45px;">GraspVLA: a Grasping Foundation Model <br>
            Pre-trained on Billion-scale Synthetic Action Data</h2>
          <!-- <h4 style="color:#6e6e6e;"> In submission </h4> -->
          <hr>
          <h6>
            <a href="https://shengliangd.github.io/about/">Shengliang Deng</a><sup>1,3*</sup>&nbsp; &nbsp;
            <a href="https://miyandoris.github.io/">Mi Yan</a><sup>1,2*</sup>&nbsp; &nbsp;
            <a href="https://songlin.github.io/">Songlin Wei</a><sup>1,2</sup>&nbsp; &nbsp;
            <a>Haixin Ma</a><sup>1</sup>&nbsp; &nbsp;
            <a>Yuxin Yang</a><sup>1</sup>&nbsp; &nbsp; 
            <a href="https://jychen18.github.io/">Jiayi Chen</a><sup>1,2</sup>&nbsp; &nbsp; <br>
            <a>Zhiqi Zhang</a><sup>1,2</sup>&nbsp; &nbsp; 
            <a>Taoyu Yang</a><sup>2</sup>&nbsp; &nbsp; 
            <a>Xuheng Zhang</a><sup>2</sup>&nbsp; &nbsp; 
            <a href="https://i.cs.hku.hk/~heming/">Heming Cui</a><sup>3</sup>&nbsp; &nbsp;
            <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a><sup>1,4†</sup>&nbsp; &nbsp;
            <a href="https://hughw19.github.io/">He Wang</a><sup>1,2,4†</sup>
            <br>
            <br>
            <p> <sup>1</sup>Galbot&nbsp; &nbsp;
              <sup>2</sup>Peking University&nbsp; &nbsp;
              <sup>3</sup>The University of Hong Kong&nbsp; &nbsp;
              <sup>4</sup>Beijing Academy of Artificial Intelligence&nbsp; &nbsp;
              <br>
            </p>
            <p>
              <sup>†</sup> corresponding author &nbsp;
              <br>
            </p>

            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/pdf/2505.03233" role="button" target="_blank" style="background-color: #d3d3d3d2;color: black">
                    <i class="fa fa-file"></i>Paper</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/GraspVLA" role="button" target="_blank" disabled=1 style="background-color: #d3d3d3d2;color: black">
                <i class="fa fa-github-alt"></i>Code</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/GraspVLA?tab=readme-ov-file#model-server" role="button" target="_blank" disabled=1 style="background-color: #d3d3d3d2;color: black">
              <i class="fa fa-github-alt"></i>Checkpoint</a> </p>
            </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" role="button" target="_blank" disabled=1 style="background-color: #d3d3d3d2;color: black">
              <i class="fa fa-github-alt"></i>Dataset (Coming Soon)</a> </p>
              </div>
            </div>

            <!-- <div>
              <br>
              <br>
              <strong>CVPR 2024</strong>
            </div> -->

        </div>
      </div>
    </div>
  </div>
</section>

<!-- teaser -->
<section>
  <div class="container" style="width:70%" id="teaser">
    <img src="images/teaser.jpg" width="100%"> 
  </div>
</section>
<br>

<!-- abstract -->
<section>
  <div class="container" style="width:70%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Abstract</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action (VLA) models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.
        </p>
      </div>
    </div>
  </div>
</section>
<br>

<!-- zero-shot -->
<section>
  <div class="container" style="width:70%" id="zero-shot-video">
    <div class="row">
      <div class="col-12">
        <h2><strong>Zero-Shot Evaluation</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          Pre-trained on our billion-scale dataset, GraspVLA demonstrates strong zero-shot generalizability across 6 aspects, including distractor, spatial pose, category, lighting, background, and close-loop actions.
        </p>

        <h5><strong>1. Generalization to distractors</strong></h5>
        <p class="text-justify">
          Cluttered scenes with 30+ distractors.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/1-dense_.mp4" type="video/mp4">
          </video>
        </div>
        <br>
        <p class="text-justify">
          Dynamic distractors.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/1-distractor_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>2. Generalization to lighting variations</strong></h5>
        <p class="text-justify">
          Various lighting conditions. In the second video, even when the object is moved to a new location under dark lighting conditions, GraspVLA can still track and grasp it.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/2-light_.mp4" type="video/mp4">
          </video>
        </div>
        <br>
        <br>

        <h5><strong>3. Generalization to spatial variations</strong></h5>
        <p class="text-justify">
          Balls at different heights.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/3-height_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <p class="text-justify">
          Eggs with different planar poses.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/3-planar_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>4. Generalization to background variations</strong></h5>
        <p class="text-justify">
          Table with different textures.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/4-background-desk_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <p class="text-justify">
          Wall with changing colors.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/4-background-wall_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>5. Generalization to categories</strong></h5>
        <p class="text-justify">
          Co-trained with grounding tasks using Internet data, GraspVLA can generalize to novel categories without any action label.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/5-category_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>6. Closed-loop control</strong></h5>
        <p class="text-justify">
          GraspVLA can automatically make closed-loop adjustments in response to disturbances until the task is completed.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="40%">
            <source src="videos/6-closed-loop_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>

<!-- post-train -->
<section>
  <div class="container" style="width:70%" id="zero-shot-video">
    <div class="row">
      <div class="col-12">
        <h2><strong>Efficient post-training</strong></h2>
        <hr style="margin-top:0px">

        <h5><strong>1. Industry: new vocabulary</strong></h5>
        <p class="text-justify">
          In industrial scenarios, although our pre-trained GraspVLA can grasp any part, it struggles to identify parts with special names.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="40%">
            <source src="videos/industry-fail_.mp4" type="video/mp4">
          </video>
        </div>
        <br>
        <p class="text-justify">
          A few data with only bounding box annotation helps GraspVLA master all the rare parts.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/industry_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>2. Retail: novel action patterns</strong></h5>
        <p class="text-justify">
          In retail scenario, with a few trajectories on one kind of bottles, GraspVLA learns to sequentially pick up bottles from a densely-packed environment. This behavior can also be transferred to bottles unseen in post-training.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="90%">
            <source src="videos/retail_.mp4" type="video/mp4">
          </video>
        </div>
        <br>

        <h5><strong>3. Home: align with human preference</strong></h5>
        <p class="text-justify">
          GraspVLA learns to grasp mugs with specific grasping pose preference from a few demonstrations, and generalize to new mugs.
        </p>
        <div class="row justify-content-center" style="align-items:center; display:flex">
          <video id="zero-shot-video" autobuffer muted autoplay loop width="60%">
            <source src="videos/home_.mp4" type="video/mp4">
          </video>
        </div>
        <br>


      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>

<!-- SynGrasp-1B -->
<section>
  <div class="container" style="width:70%" id="zero-shot-video">
    <div class="row">
      <div class="col-12">
        <h2><strong>SynGrasp-1B</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          We introduce SynGrasp-1B, a billion-frame grasping dataset in simulation, featuring photorealistic rendering and extensive domain randomization, including initial robot pose, object pose, background, lighting, and material. 
        </p>
        <direct class="row justify-content-center" style="align-items:center; display:flex">
        <video id="zero-shot-video" autobuffer muted autoplay loop controls width="90%">
          <source src="videos/simdata.mp4" type="video/mp4">
        </video>
        
        <p class="text-justify">
          Data generation pipeline: We select over 10,000 object meshes from 240 categories in Objaverse and randomly place them on the table (left). We then use BoDex to generate stable grasps for each object and use CuRobo to plan grasping trajectories (middle). Finally, we apply domain randomization to materials, lighting, camera views, and backgrounds to simulate and render the trajectories (right). 
        </p>
        <div class="container" style="width:90%" id="teaser">
          <img src="images/datagen.png" width="100%"> 
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>


<!-- Model Architecture -->
<section>
  <div class="container" style="width:70%" id="zero-shot-video">
    <div class="row">
      <div class="col-12">
        <h2><strong>Model</strong></h2>
        <hr style="margin-top:0px">        
        <p class="text-justify">
          GraspVLA consists of an autoregressive vision-language backbone and a flow-matching based action expert. It exploits the synergy between Internet grounding data and synthetic action data with a Progressive Action Generation mechanism: the model first predicts 2D bounding boxes of the target object for both synthetic data and web data, and additionally generates grasp pose and chunked actions for synthetic data.
        </p>
        <div class="container" style="width:90%" id="teaser">
          <img src="images/pipeline.png" width="100%"> 
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<br>


<!-- Contact -->
<div class="container" style="width:70%">
  <div class="row ">
    <div class="col-12">
      <h2><strong>Contact</strong></h2>
      <hr style="margin-top:0px">
      <p>If you have any questions, please feel free to contact <b>Shengliang Deng</b> at sldeng@cs.hku.hk and <b>Mi Yan</b> at dorisyan@pku.edu.cn.
      </p>
      </pre>
    </div>
  </div>
</div>

<footer class="text-center" style="margin-bottom:10px; font-size: medium;">
  <hr>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a
    href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
</footer>

</body>
</html>